\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{fullpage}
\usepackage{algorithm}
\usepackage{algorithmic}


\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{cor}{Corollary}

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\fY}{\field{Y}}
\newcommand{\fX}{\field{X}}
\newcommand{\fH}{\field{H}}
\newcommand{\R}{\field{R}}
\newcommand{\Nat}{\field{N}}
\newcommand{\E}{\field{E}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\bp}{\boldsymbol{p}}
\newcommand{\bg}{\boldsymbol{g}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\br}{\boldsymbol{r}}

\title{Notes on Betting Potentials and Their Approximations}
\author{Francesco Orabona}
\date{April 2019}

\begin{document}

\maketitle

\section{Bayesian Potential}

Here we define a Bayesian potential for betting on the outcome of a ``continuous coin''.

Define the outcome at time $t$ of the continuous coin as $g_t \in [-1,1]$.
Define a potential function of the form
\[
f(g_1, \dots, g_{t-1})=E_{\beta} \left[\prod_{i=1}^{t-1}(1+\beta g_i) \right],
\]
where $\beta$ is drawn from a distribution with support $[-1,1]$.
Note that this is more general that the definition of the potential function in the paper that only depends on the sum of the outcomes of the coin and time.

Let's equip such potential function with the following betting fraction
\[
\beta_t 
= \frac{E_\beta[\beta \prod_{i=1}^{t-1}(1+\beta g_i)]}{E_\beta[\prod_{i=1}^{t-1}(1+\beta g_i)]}
= \frac{E_\beta[\beta \prod_{i=1}^{t-1}(1+\beta g_i)]}{f(g_1, \dots, g_{t-1})}~.
\]
This betting fraction comes from the formula $\beta_t=\frac{f(g_1, \dots, g_{t-1},+1)-f(g_1, \dots, g_{t-1},-1)}{f(g_1, \dots, g_{t-1},+1)+f(g_1, \dots, g_{t-1},-1)}$ that is a direct generalization of the one in the paper.

{\bf Yoav:} The last equation for $\beta_t$ seems to be specific for $g_t \in \{-1.+1\}$ not for $g_t \in [-1,+1]$. I believe that in the second case $\beta_t = (\sum g_t)/(\sum g_t^2)$. What would be the potential $F$ be in that case? Can you point me to the place where you derive the potential from the expectation over $\beta$ according to the dirichlet (1/2,1/2) ?

It is easy to show by direct calculation that we have
\[
f(g_1,\dots, g_t)=f(g_1, \dots, g_{t-1})(1+g_t \beta_t)~.
\]
Moreover, by induction, we can prove that the wealth at time $t$ is exactly equal to $f(g_1,\dots, g_t)$. In fact,

\begin{align*}
W_t
&=W_{t-1}(1+g_t \beta_t) \\
&=f(g_1, \dots, g_{t-1})\left(1+g_t \frac{E_\beta[\beta \prod_{i=1}^{t-1}(1+\beta g_i)]}{f(g_1, \dots, g_{t-1})}\right) \\
&= f(g_1, \dots, g_{t-1})+ E_\beta\left[g_t \beta \prod_{i=1}^{t-1}(1+\beta g_i)\right] \\
&=E_\beta\left[(1+g_t \beta) \prod_{i=1}^{t-1}(1+\beta g_i)\right] \\
&= f(g_1, \dots, g_{t})~.
\end{align*}



This allows us also to give a very natural interpretation to the potential function: it is the average wealth for betting fraction drawn according to the chosen distribution.

KT comes from the potential function above, where the distribution over beta is proportional to $\frac{1}{\sqrt{(1-\beta)(1+\beta)}}$ over $(-1,1)$. 

In particular, we have (from Wolfram Alpha)
\[
\int_0^1 x^a (1-x)^b dx
= \frac{\Gamma(1 + a) \Gamma(1 + b)}{\Gamma(2 + a + b)}~.
\]
{\bf Yoav:} I think there might be a mistake here. When you go from $\prod_{i=1}^{t-1}(1+\beta g_i)]$ to $x^a (1-x)^b$. you need to account for the different permutations that give you $a$ and $b$ which are something like ${a+b \choose a}$.

With the change of variable $\beta=2x-1$, we have
\[
\frac{1}{2}\int_{-1}^1 \left(\frac{\beta+1}{2}\right)^a \left(\frac{1-\beta}{2}\right)^b d\beta
= \frac{\Gamma(1 + a) \Gamma(1 + b)}{\Gamma(2 + a + b)}~.
\]
Now observe that the KT potential for binary coins we have
\[
\int_{-1}^1 \frac{1}{\pi} (1+\beta)^{a-\frac{1}{2}} (1-\beta)^{b-\frac{1}{2}} d\beta
= \frac{2^{a+b} \Gamma(\frac{1}{2} + a) \Gamma(\frac{1}{2} + b)}{  \pi \Gamma(1 + a + b)}
\]
where $a+b$ is the total number of rounds $t$ and $a$ is the number of times we observed a $+1$. Hence, $\sum_{i=1}^t g_i= 2a-T$ that implies $a=\frac{T+\sum_{i=1}^t g_i}{2}$ and $b=\frac{T-\sum_{i=1}^t g_i}{2}$
Denoting by $x=\sum_{i=1}^t g_i$, we have
\[
\int_{-1}^1 \frac{1}{\pi} (1+\beta)^{\frac{T-1+x}{2}} (1-\beta)^{\frac{T-1-x}{2}} d\beta
= \frac{2^{t} \Gamma(\frac{T+1+x}{2}) \Gamma(\frac{T+1-x}{2})}{  \pi \Gamma(1 + t)}~.
\]

The nice closed form solution is a consequence of the choice of the distribution over $\beta$ and the fact that the $g_i$ are in $\{-1,1\}$. However, assuming we could solve the expectation, we could still use the same potential in the case of $g_i$ in $[-1,1]$. 

% Such strategy would work because we already proved that it is a viable betting potential. Also, maybe the above potential can be extended to the continuous time setting, even if I don't know how...

\subsection{Continuous time limit}
To define the potential in continuous time suppose that we subdivide each unit time
into $S$ equal parts. We want to see how to scale $g_t$ and $x$ to get a meaningful limit. We start with the expression:
$$ (1+g\beta)^{\frac{S(T+x)}{2}} (1-g\beta)^{\frac{S(T-x)}{2}} $$
We can rewrite this expression as
$$ ((1+g\beta)(1-g\beta))^{\frac{ST}{2}} \left( \frac{1+g\beta}{1-g \beta}\right)^{\frac{Sx}{2}}
=
(1-g^2 \beta^2)^{\frac{ST}{2}} \left( \frac{1+g\beta}{1-g \beta}\right)^{\frac{Sx}{2}}
$$

Consider the first factor:$$f_1=(1-g^2 \beta^2)^{\frac{ST}{2}}$$. Keeping $\beta$ and $T$ constant we let $S$ increase to infinity and consider how $g$ depends on $S$.  
If $g = o(1/\sqrt{S})$, then $f_1 \to 0$, while if $1/\sqrt{S} = o(g)$ then $f_1 \to \infty$. Only if $g = \Theta(1/\sqrt{S})$ we get that $f_1$ has a non-trivial limit.
Absorbing any additional constants into $\beta$ we set $g = 1/\sqrt{S}$ and get:

$$ \lim_{S \to \infty} f_1 = \lim_{S \to \infty} \left( \left(1-\frac{\beta^2}{S}\right)^S \right)^{T/2} = e^{-\beta^2 T / 2}$$

Consider next the second factor $$f_2 = \left( \frac{1+g\beta}{1-g \beta}\right)^{\frac{Sx}{2}} = \left( \frac{1+\beta/\sqrt{S}}{1- \beta/\sqrt{S}}\right)^{\frac{Sx}{2}}$$

In order that $f_2$ converges as $S \to \infty$ we set $x = \alpha T / \sqrt{S}$.
Plugging in this choice and letting $S \to \infty$ we get: 
\[
\lim_{S \to \infty} \left( \frac{1+\beta/\sqrt{S}}{1- \beta/\sqrt{S}}\right)^{\frac{\sqrt{S} \alpha T}{2}} = 
\lim_{S \to \infty} \left(\left( 1+2 \beta/\sqrt{S}\right)^{\sqrt{S}} \right)^{\frac{T \alpha}{2}}
=e^{\beta\alpha T }
\]

Taking the product of the limits of $f_1$ and $f_2$ we get
\[
W(\alpha,\beta,T) = \exp \left( \beta \alpha T - \frac{T}{2} \beta^2 \right)  =
\exp \left( \frac{T}{2} \left( 2 \beta \alpha - \beta^2 \right) \right) =
\exp \left( -\frac{T}{2} \left( \alpha - \beta \right)^2 + \frac{T}{2} \alpha^2 \right)
\]
The regret (ratio) is:
\[
R(\alpha,\beta,T) = \frac{W(\alpha,\beta,T)}{W(\alpha,\alpha,T)} = 
\frac{\exp \left( -\frac{T}{2} \left( \alpha - \beta \right)^2 + \frac{T}{2} \alpha^2 \right)}{\exp \left( \frac{T}{2} \alpha^2 \right)} = \exp \left( -\frac{T}{2} \left( \alpha - \beta \right)^2 \right)
\]

$R(\cdot,\beta,\cdot)$ is (up to a normalization factor) a Gaussian distribution centered at $\alpha$.

Considering a pseudo Bayesian algorithm that uses the uniform distribution over the real line as a "pseudo-prior". The regret of this algorithm is
\[\int_{-\infty}^{+\infty} R(\alpha,\beta,T) d\beta
= \int_{-\infty}^{+\infty} \exp \left( -\frac{T}{2} \left( \alpha - \beta \right)^2 \right) d\beta
= \sqrt{\frac{T}{8 \pi}}
\]

As the integral does not depend on $\alpha$, the uniform pseudo-distribution is the best choice.

\section{back to finite}

Once we found what is the continuous time potential, we use that potential to define our algorithm (we already know what $\beta$ will be. For finite steps the potential will increase a little bit at each iteration, and that will give us our regret bound that depends on $\sum_{t=1}^T g_t^2$ instead of $T$.


\section{Bayesian Potential in Continuous time}

In this section we replace $f(\cdots)$ with $h(\cdots) \doteq \log(f(\cdots))$

As $f(g_1,\ldots,g^T) = \prod_{t=1}^T (1+\beta_t g_t)$ we have that 
$h(g_1,\ldots,g^T) = \sum_{t=1}^T \log(1+\beta_t g_t)$


\newcommand{\dt}{\Delta t}
In order to calculate the potential for continuous time, we define a time step $\dt>0$ and replace the times $t=0,1,2,3\ldots,T$ by $\tau=0,\dt,2\dt,\ldots,S$. $\dt=1/T$.

as $T$ increases we need to reduce the range of $g_t$. Clearly $|g_t| \leq \dt$ works, but gives trivial bounds.I strongly believe, based on past work, that the right scaling is $|g_t| \leq \sqrt{\dt}$. It is clearer if we keep the range $[-1,+1]$ for $g_t$ and multiply by $\sqrt{\dt}$

We rewrite the definition of $h$ as 
\[h(g(0),g(1/\dt),\ldots,g(1)) = \sum_{t=1}^T \log(1+\beta(t\dt) g(t\dt)\sqrt{\dt})
\]
Next, we set $\dt = 1/T$ and let $T \to \infty$. Using the Taylor series 
$\log(1+x) = x-\frac{x^2}{2} +o(x^2)$ we get that 
\[
\log(1+\beta_t g_t) \approx \beta_t g_t - \frac{\beta_t^2 g_t^2}{2}
\]


This yields
\[
h(g(0),\ldots,g(S)) \approx 
\sum_{t=0}^{T} 
\left[\sqrt{\dt}(\beta(t \dt) g(t\dt) - \frac{\dt}{2}(\beta(t \dt) g(t\dt))^2\right]
\]

\iffalse
\subsection{stochastic sequence interpretation}
As $t \to \infty$ we can think of the exponent of the potential as a stochastic Ito process: 

$$h(\tau) = \int_0^t \beta g(\tau)\,dB_\tau + \int_0^t \beta g(\tau)  \,ds.$$
Where the second integral is the standard Lebesgue-Stiljes integral and the first is an integral wrt the standard Brownian process. The potential can then be written as another stochastic process:
\[
f(\tau) = E_{\beta} \left[  \exp(h(\tau)) \right]
\]

This potential is similar looking to the squint potential but here we add $\frac{1}{2}\beta^2g^2$ while in squint we subtract $\beta^2 g^2$.

My plan to make this into a bound for the standard discrete time is to compute a bound on the difference between the discrete bound and the continuous bound. This bound will give, I expect, a lower order term as $t \to \infty$.
\fi

\subsection{Calculating a Lower Bound to the Final Wealth}
Given the above definition, we need only to calculate a lower bound to the wealth in the last round. If we don't mind about getting a somewhat loose lower bound, we can easily get such lower bound in the following way.
First we need a technical lemma.
\begin{lemma}
\label{lemma:l1}
Let $f(\eta)=-\log(1-|\eta|)-|\eta|$, for $-1 \leq \eta\leq 1 $. Then 
\[
f^*(x)=\max_\eta \eta x - f(x) = \max_{-1\leq \eta\leq 1} \eta x - f(x) = |x|-\log(|x|+1)\geq \frac{x^2}{2(|x|+1)}~.
\]
\end{lemma}
%
\begin{proof}
By definition,
\[
f^*(x)=\max_{\eta} \ \eta x + \log(1-|\eta|)+|\eta|
\]
We equal the derivative wrt $\eta$ to zero to find the minimizer
\[
x-\frac{sign(\eta^*)}{1-|\eta^*|}+sign(\eta^*)=0
\]
From which we have that
\[
\eta^*=\frac{x}{|x|+1}~.
\]
Notice that $|\eta^*|\leq 1$, hence $\max_{\eta} \ \eta x -f(\eta) = \max_{-1\leq \eta \leq 1} \ \eta x -f(\eta)$.
Hence
\[
f^*(x)= \frac{x^2}{|x|+1} - \log(|x|+1)+\frac{|x|}{|x|+1} = |x|-\log(|x|+1)
\]
The second lower bound is proved comparing the derivatives.
\end{proof}

Now, assume without loss of generality that $\sum_{i=1}^T g_i\geq0$ and define $\hat{\beta}=\frac{\sum_{i=1}^T g_i}{\sum_{i=1}^T g_i^2 + |\sum_{i=1}^T g_i|}$. Define the prior over $\beta$ as $P$ and $Q$ proportional to $P$ in $[\alpha \hat{\beta},\hat{\beta}]$ and 0 otherwise, where is $\alpha$ is a free positive constant less than 1.
Using the change of measure lemma, we have
\begin{align*}
f(g_1, \dots, g_{T})
&=E_{\beta\sim P} \left[\prod_{i=1}^{T}(1+\beta g_i) \right] \\
&=E_{\beta\sim P} \left[\exp\left(\sum_{i=1}^{T}\ln(1+\beta g_i)\right) \right] \\
& \geq \exp\left(E_{\beta\sim Q} \left[\sum_{i=1}^T \ln(1+\beta g_i) \right]-KL(Q,P)\right),
\end{align*}
where $KL(Q,P) = E_{h\sim Q}[\ln \frac{Q(h)}{P(h)}]$.
Given the definition of $Q$, we have
\[
KL(Q,P) = -\ln \int_{\alpha \hat{\beta}}^{\hat{\beta}} P(\beta) d\beta~.
\]
Notice that $\frac{\log(1+\beta x)-\eta x}{\beta^2 x^2} \geq \frac{\log(1-|\beta|)+|\beta|}{\beta^2}$ for $|x|\leq 1$, hence
$\log(1+\beta x)\geq \beta x + x^2 [\log(1-|\beta|)+|\beta|]$. So, we obtain
\begin{align*}
\E_{\beta\sim Q}\left[\sum_{i=1}^{T} \log(1+\beta g_i)\right]  
&\geq \E_{\beta\sim Q_t} \left[\beta \sum_{i=1}^T g_i + [\log(1-|\beta|)+|\beta|]\sum_{i=1}^T g_i^2\right]~.
\end{align*}

Define $\Psi(x)=|x|-\log(|x|+1)$.
Now, from Lemma~\ref{lemma:l1}, we use the fact that the expression in the expectation in rhs is concave wrt to $\beta$ and the max is attained in $\hat{\beta}$ to have
\begin{align*}
\E_{\beta\sim Q} \left[\beta \sum_{i=1}^T g_i + [\log(1-|\beta|)+|\beta|]\sum_{i=1}^T g_i^2\right]
&\geq \E_{\beta\sim Q}\left[\alpha \hat{\beta} \sum_{i=1}^T g_i + [\log(1-|\alpha\hat{\beta}|)+|\alpha\hat{\beta}|]\sum_{i=1}^T g_i^2\right]\\
&= \alpha \hat{\beta} \sum_{i=1}^T g_i + [\log(1-|\alpha\hat{\beta}|)+|\alpha\hat{\beta}|]\sum_{i=1}^T g_i^2\\
&\geq \alpha (\hat{\beta} \sum_{i=1}^T g_i + [\log(1-|\hat{\beta}|)+|\hat{\beta}|]\sum_{i=1}^T g_i^2)\\
&= \alpha \sum_{i=1}^T g_i^2 \left(\hat{\beta} \frac{\sum_{i=1}^T g_i}{\sum_{i=1}^T g_i^2} + [\log(1-|\hat{\beta}|)+|\hat{\beta}|]\right)\\
&= \alpha \Psi\left(\frac{|\sum_{i=1}^T g_i|}{\sum_{i=1}^T g_i^2}\right) \sum_{i=1}^T g_i^2,
%&\geq \alpha\frac{\left(\sum_{i=1}^t g_i\right)^2}{2\left(\left|\sum_{i=1}^t g_i\right|+ \sum_{i=1}^t g_i^2\right)},
\end{align*}
where in the second inequality we used the fact that $\frac{\log(1-x|\beta|)+x|\beta|}{x}\geq \log(1-|\beta|)+|\beta|$ because from the first derivative $\frac{\log(1-x|\beta|)+x|\beta|}{x}$ is decreasing for $x \in (0,1]$, and in the last equality we used Lemma~\ref{lemma:l1}.

Putting all together we have
\begin{align*}
W_T &= \E_{\beta\sim P}\left[\prod_{i=1}^{T} (1+\beta g_i)\right] \\
&\geq \exp\left(\alpha \Psi\left(\frac{|\sum_{i=1}^T g_i|}{\sum_{i=1}^T g_i^2}\right) \sum_{i=1}^T g_i^2 + \ln \int_{\alpha \hat{\beta}}^{\hat{\beta}} P(\beta) d\beta\right) \\
&\geq \exp\left(\alpha \frac{(\sum_{i=1}^T g_i)^2}{2 (\sum_{i=1}^T g_i^2+|\sum_{i=1}^T g_i|)} + \ln \int_{\alpha \hat{\beta}}^{\hat{\beta}} P(\beta) d\beta\right)~.
\end{align*}

Basically, the wealth will have the right shape and the price we pay for not knowing in advance the optimal betting fraction depends on how much probability mass our prior places around the (approximated) optimal betting fraction.

\subsection{Approximating the Bayesian Potential: Squint Potential}

In the general case, the potential defined in the previous section cannot be calculated in a closed form. So, here we present an approximation that in some cases gives us a closed form solution. 
Consider potentials of the form
\begin{equation}
\label{eq:approx_potential}
f(g_1, \dots, g_{t-1})=E_{\beta} \left[\exp\sum_{i=1}^{t-1}(\beta g_i-\beta^2 g_i^2) \right],
\end{equation}
where $\beta$ is drawn from a distribution with support $[-\frac12, \frac12]$.
This coin-betting potential is hidden in the details of the Squint paper.

It might not be immediate, but this potential is nothing else than a second order approximation of the $\ln(1+x)$ function. In fact, it comes from this inequality 
\begin{equation}
\label{eq:ineq}
1+g_i \beta \geq \exp(\beta g_i - \beta^2 g_i^2), \ \forall \beta \in [-1/2,1/2],
\end{equation}
that applied over the products in the Bayesian potential gives the potential in \eqref{eq:approx_potential}.

Associate the betting fraction equal to
\[
\beta_t = \frac{E_{\beta} \left[\beta \exp \sum_{i=1}^{t-1}(\beta g_i-\beta^2 g_i^2) \right]}{f(g_1, \dots, g_{t-1})}~.
\]
By induction, we can prove that the wealth at time $t$ is lower bounded by $f(g_1, \dots, g_{t})$. Note that the fact that we are using an approximation gives us only a lower bound rather than an equality.
The proof goes as follows
\begin{align*}
W_t
&=W_{t-1}(1+g_t \beta_t) \\
&\geq f(g_1, \dots, g_{t-1})\left(1+g_t \frac{E_\beta[\beta \exp \sum_{i=1}^{t-1}(\beta g_i-\beta^2 g_i^2)]}{f(g_1, \dots, g_{t-1})}\right) \\
&= f(g_1, \dots, g_{t-1})+ E_\beta\left[g_t \beta \exp \sum_{i=1}^{t-1}(\beta g_i-\beta^2 g_i^2)\right] \\
&=E_\beta\left[(1+g_t \beta) \exp \sum_{i=1}^{t-1}(\beta g_i-\beta^2 g_i^2)\right] \\
&\geq E_\beta\left[\exp \sum_{i=1}^{t}(\beta g_i-\beta^2 g_i^2)\right] \\
&= f(g_1, \dots, g_{t}),
\end{align*}
where in the last inequality we have used \eqref{eq:ineq}.

So, this potential too is a feasible one and given its shape there is hope to calculate it in a closed form. Also, now we are ``summarizing'' the coins with the statistics of the sum of the $g_i$ and $g_i^2$. Note that the coefficient in front of the $g_i^2$ is not exactly the one coming from a Taylor approximation of the log, but it is probably not a big deal.

Note that in principle, we could refine our approximation considering even higher order terms, and assuming we can still compute the betting fraction in a closed form.

Even in this case, we only need to calculate a lower bound to the wealth in the last round, that depends on the choice of the distribution over $\beta$.

\section{How to Improve Squint?}

I believe that both the Bayesian potential and its approximation seem to be the right ones, assuming we have a good prior. Also, given that Squint already uses the approximated potential, I suspect that in order to get a better second order bound for the learning with expert setting, we need to construct a different reduction from learning with expert advice to coin-betting. The current reduction, the one in AdaNormalHedge and the one in Squint, all heavily hinge on constructing the outcomes of the coins with the instantaneous regret of the algorithm. It is not clear to me this is the right thing to do: I just used it because it worked and because something similar is used in the book of Gabor\&Nicolo'. 

\section{New Thoughts}

The potential used in NormalHedge, AdaNormalHedge, KT is roughly of the form
\[
\phi_{t-1}(x) = \exp\left( \frac{x^2}{4 t} - \sum_{i=1}^t \frac{1}{i}\right)~.
\]

This potential can be rewritten as
\[
\phi_{t-1}(x) = \max_{\beta} \ \left(\psi_{t-1}(x,
\beta):=\exp\left( \beta x -\beta^2 t - \sum_{i=1}^t \frac{1}{i}\right)\right)~.
\]
Rewriting the potential in this way allows us to quickly prove that this potential works. Also, it might give us a new way to get potentials that depends on $\sum_{t=1}^T g_t^2$.

First, let's see why this works. We have to show for a suitable choice of $\beta_t$ that
\[
\phi_{t-1}(x)(1+\beta_t g) \geq \phi_t(x+g),
\]
where $|g_t|\leq1$. Here, we will pick $\beta_t=\arg\max_\beta \ \psi_{t-1}(x,\beta) = \frac{x}{2t}$.
So, we have
\begin{align*}
\phi_{t-1}(x)(1+\beta_t g) 
&=\psi_{t-1}(x,\beta_t) (1+\beta_t g) \\
&=\exp\left( \beta_t x -\beta_t^2 t - \sum_{i=1}^t \frac{1}{i}\right) (1+\beta_t g) \\
&\geq \exp\left( \beta_t (x+g) -\beta_t^2 (t+g^2) - \sum_{i=1}^t \frac{1}{i}\right) \\
&\geq \exp\left( \beta_t (x+g) -\beta_t^2 (t+1) - \sum_{i=1}^t \frac{1}{i}\right),
\end{align*}
where we have used the inequality $1+x\geq \exp(x-x^2), \forall x \in [-1/2,1/2]$ and the fact that $|\beta_t|\leq 1/2$.
Now observe that $\beta_{t+1}$ is the argmax of $\beta(x+g)-\beta^2(t+1)$ wrt $\beta$, hence
\begin{equation}
\label{eq:quadratic}
\beta_t (x+g) -\beta_t^2 (t+1)
= \beta_{t+1} (x+g) -\beta_{t+1}^2 (t+1) - (\beta_{t+1}-\beta_t)^2 (t+1)~.
\end{equation}
Now observe that
\begin{align*}
\left| \beta_{t+1}-\beta_t\right|
&= \left| \frac{x+g}{2 (t+1)}-\frac{x}{2 t}\right| 
= \left| \frac{x t+g t - x t -x}{2 t(t+1)}\right| 
\leq \frac{|g| t +|x|}{2 t(t+1)} 
\leq \frac{|g| t +t}{2 t(t+1)} 
\leq \frac{1}{t+1},
\end{align*}
where we used the fact that $|x|\leq t$.

Hence, putting all together, we have
\begin{align*}
\phi_{t-1}(x)(1+\beta_t g) 
&\geq \exp\left( \beta_t (x+g) -\beta_t^2 (t+1) - \sum_{i=1}^t \frac{1}{i}\right) \\
&\geq \exp\left( \beta_{t+1} (x+g) -\beta_{t+1}^2 (t+1) - \frac{1}{t+1}- \sum_{i=1}^t \frac{1}{i}\right) \\
&= \exp\left( \beta_{t+1} (x+g) -\beta_{t+1}^2 (t+1) - \sum_{i=1}^{t-1} \frac{1}{i}\right) \\
&= \phi_{t}(x+g)~.
\end{align*}

Now the next steps: The above works because it makes use of the fact that the particular shape of the potential function can be written as the maximization of the exponential of a polynomial, and that polynomial works perfectly with the key inequality $1+x\geq \exp(x-x^2), \forall x \in [-1/2,1/2]$. Note that $g^2$ appears in the proof, but we upper bound it with 1. Could we keep it around with this approach? Probably not, for two reasons: 1) the expression $\beta_t=\frac{x}{\sum_{i=1}^t g_i^2}$ can be bigger than 1 in absolute value; 2) even changing the formula of $\beta_t$ a potential of the form $\exp(\frac{x^2}{\sum_{i=1}^t g_i^2})$ cannot be the right one. To see this last point, consider the case that $g_i=g$ for all $i=1,\dots, t$. In this case, the optimal constant betting strategy (our competitor) would be always a fraction of one with sign equal to the sign of $g$ and it would obtain a wealth of 
\[
\prod_{i=1}^t (1+|g|) 
= \exp( t \log(1+|g|)) 
\approx \exp(|g| t)~. 
\]
On the other hand, if a potential of the form $\exp(\frac{x^2}{\sum_{i=1}^t g_i^2})$ were possible, in the same situation we would obtain a wealth of the form
\[
\exp(\frac{(\sum_{i=1}^t g_i)^2}{\sum_{i=1}^t g_i^2})
= \exp(\frac{g^2 t^2}{t g^2})
= \exp(t),
\]
that can be arbitrarily bigger than the wealth of the competitor. This seems impossible!

So, to adapt the above proof to obtain $g_i^2$ terms, I need a new inequality. The inequality $1+x\geq \exp(x-x^2), \forall x \in [-1/2,1/2]$ is not powerful enough. In particular, I need an inequality that gives rise to an expression that when maximized wrt to $\beta$ always give a number between -1 and 1. In this way, the proof above would naturally give me the right potential and the right betting fraction.
I claim that the right inequality is the following one:
\[
1+\beta g \geq \exp\left(\beta g + g^2\left(\log(1-|\beta|)+|\beta|\right)\right)~.
\]
The above should work for any $\beta$ and $g$ in $[-1,1]$, even if I proved it sometime ago and I don't have the proof anymore...
I claim the above gives rise to $\beta_t$ that are always between $-1$ and $1$. Also, it allows to use the $g_i^2$. The only part of the proof that should be worked out is the step in \eqref{eq:quadratic}, because the expression is not a quadratic anymore. However, it behaves like a quadratic, in particular by strong convexity and smoothness I think it should work.

\section{From betting to experts}

1d learner: input is $\hat{g}_t \in[-1,1]$, output is $w_t$, objective is to maximize $\sum_{t=1}^T w_t \hat{g}_t$.

Hyp: $\sum_{t=1}^T w_t \hat{g}_t \geq F(\sum_{t=1}^T \hat{g}_t)$.

Expert prediction: probability distribution $\boldsymbol{p}_{t,i}$ is $\propto \pi_i \max(w_{t,i},0)$.

Receive the gains $\boldsymbol{g}_t$.

Construct $\hat{g}_{t,i}$: $\hat{g}_t \approx g_{t,i} - \langle \boldsymbol{g}_t, \boldsymbol{p}_t\rangle$ (correct assuming $\bw_{t,i}>0$)

Claim:
Let's define $f(x)=\ln(F(x))$
\[
Regret(\boldsymbol{q}) \leq f^{-1}\left(KL\left(\boldsymbol{q},\bodlsymbol{\pi}\right)\right)
\]

KT, exp(x^2/T): $f^{-1}(x)=\sqrt{T x}$

Potential of 1d learner is $\exp(\eta x)$,  $f^{-1}(x)=x/\eta$


Short proof using Fenchel dual of KL divergence (that is the log partition function):
Assume $\log F(x)$ to be convex

\begin{align*}
\log F(\langle \bu, \boldsymbol{G}\rangle)
\leq \sum_{i=1}^N u_i \log F(G_i)
\leq KL(\bu \| \boldsymbol{\pi}) + \log\left(\sum_{i=1}^N \pi_i F(G_i)\right)~.
\end{align*}
Note that the entire reduction makes $\sum_{i=1}^N \pi_i F(G_i)\leq 1$. Hence, we have
\[
\log F(\langle \bu, \boldsymbol{G}\rangle) 
\leq KL(\bu \| \boldsymbol{\pi})~.
\]
Setting $G_i$ such that $G_i \geq \sum_{t=1}^T (g_{t,i}-\langle \bp_t, \bg_t\rangle)$ we have that $Regret(\bu) \leq \langle \bu, \boldsymbol{G}\rangle$.


Can I say that it is enough to have any choice of $\bp_t$ that makes $\log\left(\sum_{i=1}^N \pi_i F(G_i)\right)$ decrease monotonically?

Note that the average money of the bettors on the arms monotonically decreases. Note that it is not simply that $\tilde{g}_{t,i}$ has opposite sign of $w_{t,i}$.

It seems I get the normalization by division of L1, by the following
\[
\btheta \rightarrow \log |\btheta|_+ \rightarrow \arg\max_{\bp} \langle \btheta, \bp\rangle - \sum_{i} p_i \log p_i~.
\]
Check that the same works for Lp norms.
Also, in the proof above I take log of potential, not of theta, even if the potential gives the predictions.

\section{Random considerations: work in progress}

We are trying to find a better way to analyze the bandit algorithm with Tsallis entropy. The hope is that such understanding will shed light on the reduction between 1d OLO and experts/bandits.
The optimal one is obtained with the regularizer $\psi(\bx)=-\sum_{i=1}^N \sqrt{x_i}+\boldsymbol{1}_{\Delta^{N-1}}(\bx)$. This regularizer effectively puts a barrier on 0. Also, the constraint makes the dual not in a close form.

What happens if I use a regularizer of the form $-\sqrt{x(1-x)}$ on each arm and then find a way to normalize the predictions? This puts a barrier on both $0$ and $1$. Reasoning: The entropy function has a similar shape on each coordinate, that is $x \ln(x)$, and it makes the regularizer not decreasing monotonically.

This regularizer is 2-strongly convex on $[-1,1]$, and the curvature increases around 0 and 1.

The argmax of the maximization to find the dual is $\frac{1}{2}+\frac{\theta}{2\sqrt{\theta^2+1}}$, a sigmoid transfer function that maps $\R$ to $[0,1]$.
The dual is $\psi^\star(\theta)=\frac{1}{2} \left(\theta + \sqrt{1 + \theta^2} \right)$. It looks like an inverted and smoothed hinge loss.


Setting $\psi(x)=-\eta\sqrt{x(1-x)}$, we have $ \psi^\star(\theta) = \frac{1}{2} \left(\theta + \sqrt{\eta^2+\theta^2}\right)$, and $\lambda = 2\eta$.
So, overall we have
\[
\langle \theta_T,\bu\rangle
\leq \eta\left(something-\sum_{i=1}^N \sqrt{x(1-x)}\right)+\frac{1}{4\eta}\sum_{t=1}^T \|\br_t\|^2_2,
\]
where ``something'' depends on $F(0)=N \psi^\star(0)$? It should be $\sqrt{N}$. Probably better express it in terms of the primal, to evaluate it over a distribution.

How do we obtain the better bound? We have to sharpen the smoothness bound, using the second derivative of the potential. Note that the original proof requires the losses to be negative, while here we use the instantaneous regret.

However, maybe I can make the update invariant to shifts in $\theta_i$, for example, subtracting the average $\theta_i$ from each of them.
The regret is independent of scaling of the losses by constant terms on each round, because $\bu$ and $\bp_t$ are distributions:

Hence, I want the algorithm to have the same property! Note that the instantaneous regret gives me this property for free. So, I can consider in the analysis the presence of $-\langle \bp_t, \boldsymbol{\ell_t}\rangle$ only when I need it? I can consider the smoothness inequality using zero shift, because the inner product is already negative! 
\begin{algorithm}[h]
\caption{Generic Expert Algorithm}
\label{alg:bandit}
\begin{algorithmic}
\STATE{$\btheta_0=\boldsymbol{0} \in \R^N$}
\FOR{t=1,\dots,T}
\STATE{Predict $\bp_t=\frac{\nabla F(\theta_{t-1})}{\|\nabla F(\theta_{t-1})\|_1}$, where $F(\bx)=\sum_{i=1}^N \psi^\star(x_i)$}
\STATE{Get $\boldsymbol{\ell}_t \in [-1,0]^N$}
\STATE{Set $\bg_t=\boldsymbol{\ell}_t - \langle \bp_t, \boldsymbol{\ell}_t\rangle$}
\STATE{Update $\btheta_t=\btheta_{t-1}+\bg_t$}
\ENDFOR
\end{algorithmic}
\end{algorithm}

Analysis:
Note that the algorithm above and the definition of regret are independent of uniform shifts of the loss vector:
\[
Regret_T(\bu)=
\sum_{t=1}^T \langle \boldsymbol{\ell_t}, \bu-\bp_t \rangle
= \sum_{t=1}^T \langle \boldsymbol{\ell_t}+s_t \boldsymbol{1}, \bu-\bp_t \rangle,
\]
for any shift $s_t \in \R, \ t=1,\dots,T$. We will not use this observation in the analysis.

Denote by $F(\btheta)=\sum_{i=1}^N \psi^\star(\theta_i)$. Observe that $Regret_T(\bu)=\langle \btheta_T, \bu\rangle$.
Summing over the coordinate, we have
\[
\langle \theta_T,\bu\rangle - F^*(\bu) 
\leq F(\btheta_{T}) 
\leq F(\btheta_{T-1}) + \langle \nabla F(\btheta_{T-1}), \br_{T}\rangle + \frac{1}{2\lambda} \sum_{i=1}^N r_{T,i}^2,
\]
where $\lambda$ is the strong convexity of $\psi$.
Also, set $r_{t+1}$ such that $\langle \nabla F(\btheta_{t}), \br_{t+1}\rangle= 0$, that is $r_{t,i}= \ell_{t,i}-\langle \bp_t, \ell_t \rangle$ and $\bp_t=\frac{\nabla F(\btheta_{t-1})}{\|\nabla F(\btheta_{t-1})\|_1}$.

Given that $F$ is separable, we have $F^\star(\bx)=\sum_{i=1}^N \psi(x_i)$.

The trick is the following: the strong convexity is at least 2, but it can be better when $p_{t,i}$ is close to zero.
The arm that is pulled should have a loss $\propto \frac{1}{p_{t,i_t}}$, while all the others are constants.
The Hessian for the coordinate that decreases is of the order of $p_{t,i_t}^{1.5}$. For the other coordinate, it is not negative, so the trick of Jake does not work fully, on the other hand, the minimum strong convexity is 2, can we use it?


Another observation:
\[
\arg\max \ \langle \btheta,\bx\rangle - f(\bx) 
=\arg\max \ \langle \hat{\btheta},\bx\rangle - f(\bx) 
\]
where $\hat{\btheta}=\btheta+g\bodlsymbol{1}$ and the maximization is over the simplex. This implies that any potential constructed with the constraint of the simplex give rises to updates that are independent of shifts.
\end{document}
